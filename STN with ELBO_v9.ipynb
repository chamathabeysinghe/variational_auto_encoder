{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nVariational Autoencoder with a STN and Modified Loss function and inverse STN and compare reconstruction loss\\nRemoved the classification part \\nAdd the cost function of new VAE implementation of Tensorflow documentation\\nThis is a MNIST classifier \\n\\nAdd a decoder to clip values after inverse STN \\n\\nEncoder(Localizaton network) -> CNN to predict affine transformation matrix\\nSTN -> Apply affine transformation\\nDecoder -> CNN to classify transformed images from STN\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Variational Autoencoder with a STN and Modified Loss function and inverse STN and compare reconstruction loss\n",
    "Removed the classification part \n",
    "Add the cost function of new VAE implementation of Tensorflow documentation\n",
    "This is a MNIST classifier \n",
    "\n",
    "Add a decoder to clip values after inverse STN \n",
    "\n",
    "Encoder(Localizaton network) -> CNN to predict affine transformation matrix\n",
    "STN -> Apply affine transformation\n",
    "Decoder -> CNN to classify transformed images from STN\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten, Activation, Reshape\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "from utils.data_manager import ClutteredMNIST\n",
    "from utils.visualizer import plot_mnist_sample\n",
    "from utils.visualizer import print_evaluation\n",
    "from utils.visualizer import plot_mnist_grid\n",
    "from components.STN import BilinearInterpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = \"./datasets/mnist_cluttered_60x60_6distortions.npz\"\n",
    "batch_size = 256\n",
    "num_epochs = 30\n",
    "\n",
    "data_manager = ClutteredMNIST(dataset_path)\n",
    "train_data, val_data, test_data = data_manager.load()\n",
    "x_train, y_train = train_data\n",
    "plot_mnist_sample(x_train[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "  \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "  def call(self, inputs):\n",
    "    z_mean, z_log_var = inputs\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "  \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               latent_dim=3,\n",
    "               intermediate_dim=64,\n",
    "               name='encoder',\n",
    "               **kwargs):\n",
    "    super(Encoder, self).__init__(name=name, **kwargs)\n",
    "    self.conv_1 = layers.Conv2D(20, (5, 5), padding='same', activation='relu')\n",
    "    self.max_1 = layers.MaxPool2D(pool_size=(2, 2))\n",
    "    self.conv_2 = layers.Conv2D(20, (5, 5), activation='relu')\n",
    "    self.max_2 = layers.MaxPool2D(pool_size=(2, 2))\n",
    "    \n",
    "    self.flatten = layers.Flatten()\n",
    "    \n",
    "    self.dense_1 = layers.Dense(50, activation='relu')\n",
    "    \n",
    "    self.dense_mean = layers.Dense(3)\n",
    "    self.dense_log_var = layers.Dense(3)\n",
    "    \n",
    "    self.sampling = Sampling()\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.conv_1(inputs)\n",
    "    x = self.max_1(x)\n",
    "    x = self.conv_2(x)\n",
    "    x = self.max_2(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.dense_1(x)\n",
    "    z_mean = self.dense_mean(x)\n",
    "    z_log_var = self.dense_log_var(x)\n",
    "    z = self.sampling((z_mean, z_log_var))\n",
    "    return z_mean, z_log_var, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitEncoder(layers.Layer):\n",
    "  \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               name='digit_encoder',\n",
    "               **kwargs):\n",
    "    super(DigitEncoder, self).__init__(name=name, **kwargs)\n",
    "    self.conv_1 = layers.Conv2D(20, (3, 3), padding='same', activation='relu')\n",
    "    self.max_1 = layers.MaxPool2D(pool_size=(2, 2))\n",
    "    self.conv_2 = layers.Conv2D(20, (3, 3), activation='relu')\n",
    "    self.max_2 = layers.MaxPool2D(pool_size=(2, 2))\n",
    "    \n",
    "    self.flatten = layers.Flatten()\n",
    "    \n",
    "    self.dense_1 = layers.Dense(200, activation='relu')\n",
    "    \n",
    "    self.dense_mean = layers.Dense(50)\n",
    "    self.dense_log_var = layers.Dense(50)\n",
    "    \n",
    "    self.sampling = Sampling()\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.conv_1(inputs)\n",
    "    x = self.max_1(x)\n",
    "    x = self.conv_2(x)\n",
    "    x = self.max_2(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.dense_1(x)\n",
    "    z_mean = self.dense_mean(x)\n",
    "    z_log_var = self.dense_log_var(x)\n",
    "    z = self.sampling((z_mean, z_log_var))\n",
    "    return z_mean, z_log_var, z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "  \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               name='decoder',\n",
    "               **kwargs):\n",
    "    super(Decoder, self).__init__(name=name, **kwargs)\n",
    "    self.flatten = layers.Flatten()\n",
    "    self.dense_1 = layers.Dense(3600, activation='sigmoid')\n",
    "    self.reshape = layers.Reshape((-1, 60, 60))\n",
    "\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    x = self.flatten(inputs)\n",
    "    x = self.dense_1(x)\n",
    "    x = self.reshape(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitDecoder(layers.Layer):\n",
    "  \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               name='decoder',\n",
    "               **kwargs):\n",
    "    super(DigitDecoder, self).__init__(name=name, **kwargs)\n",
    "    self.dense_1 = layers.Dense(100, activation='relu')\n",
    "    self.dense_2 = layers.Dense(3600, activation='relu')\n",
    "    self.reshape = layers.Reshape((60, 60, 1))\n",
    "\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    x = self.dense_1(inputs)\n",
    "    x = self.dense_2(x)\n",
    "    x = self.reshape(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.ones((10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.ones(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(tf.keras.Model):\n",
    "  \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_classes,\n",
    "               intermediate_dim=64,\n",
    "               latent_dim=3,\n",
    "               name='autoencoder',\n",
    "               **kwargs):\n",
    "    super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "    self.num_classes = num_classes\n",
    "    self.encoder = Encoder(latent_dim=latent_dim,\n",
    "                           intermediate_dim=intermediate_dim)\n",
    "    self.stn = BilinearInterpolation((30, 30), name='stn_layer')\n",
    "    self.digit_encoder = DigitEncoder()\n",
    "    self.digit_decoder = DigitDecoder()\n",
    "    self.re_stn = BilinearInterpolation((60, 60), name='stn_layer')\n",
    "#     self.decoder = Decoder()\n",
    "\n",
    "  def call(self, inputs):\n",
    "    z_mean, z_log_var, z_where = self.encoder(inputs)\n",
    "    n = z_where.shape[0]\n",
    "    z_where_indexes = tf.repeat([[0,0,1,0,0,2]], n, axis=0)\n",
    "    z_where_selections = tf.repeat([[1.0, 0.0 , 1.0, 0.0, 1.0, 1.0]], n, axis=0)\n",
    "    z_where_modified = tf.gather(z_where, z_where_indexes, batch_dims=1)\n",
    "    z_where_modified = tf.math.multiply(z_where_modified, z_where_selections)\n",
    "    \n",
    "    inverse_z_where =  tf.map_fn(lambda x: x/x[0], z_where)\n",
    "    inverse_z_where_compliments = tf.repeat([[1.0, -1.0 , -1.0]], n, axis=0)    \n",
    "    inverse_z_where = tf.math.multiply(inverse_z_where, inverse_z_where_compliments)\n",
    "    \n",
    "    inverse_z_where_modified = tf.gather(inverse_z_where, z_where_indexes, batch_dims=1)\n",
    "    inverse_z_where_modified = tf.math.multiply(inverse_z_where_modified, z_where_selections)\n",
    "    \n",
    "    \n",
    "    transformed_image = self.stn([inputs, z_where_modified])\n",
    "    \n",
    "    z_what_mean, z_what_log_var, z_what = self.digit_encoder(transformed_image)\n",
    "    reconstructed_crop = self.digit_decoder(z_what)\n",
    "    \n",
    "    reconstructed_image = self.re_stn([reconstructed_crop, inverse_z_where_modified])\n",
    "    \n",
    "    # Add KL divergence regularization loss.\n",
    "    kl_loss_where = - 0.5 * tf.reduce_mean(\n",
    "        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "    self.add_loss(kl_loss_where)\n",
    "    \n",
    "    kl_loss_what = - 0.5 * tf.reduce_mean(\n",
    "        z_what_log_var - tf.square(z_what_mean) - tf.exp(z_what_log_var) + 1)\n",
    "    self.add_loss(kl_loss_what)\n",
    "    \n",
    "#     clipped_image = self.decoder(reconstructed_image)\n",
    "\n",
    "    return transformed_image, [z_mean, z_log_var, z_where], [], reconstructed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "vae = VariationalAutoEncoder(num_classes, 64, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "crossentropy_loss_fn = tf.keras.losses.CategoricalCrossentropy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 128\n",
    "iterations = x_train.shape[0] // batch_size\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "mse_loss_log_fn = tf.keras.losses.MeanSquaredLogarithmicError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "  log2pi = tf.math.log(2. * np.pi)\n",
    "  return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "***********\n",
      "tf.Tensor(0.02229908, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.006838745>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0011596354>]\n",
      "tf.Tensor(0.007998381, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From /Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv2/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:644: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "step 0: mean loss = tf.Tensor(0.03029746, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.022177761, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.009318442>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0006252598>]\n",
      "tf.Tensor(0.017942082, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.022274068, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.0021997334>, <tf.Tensor: shape=(), dtype=float32, numpy=0.00040841932>]\n",
      "tf.Tensor(0.020550234, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.02166382, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.00052554626>, <tf.Tensor: shape=(), dtype=float32, numpy=0.00022991747>]\n",
      "tf.Tensor(0.021305699, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.022698216, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.00015857552>, <tf.Tensor: shape=(), dtype=float32, numpy=0.00016684283>]\n",
      "tf.Tensor(0.021631118, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021418909, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=5.497287e-05>, <tf.Tensor: shape=(), dtype=float32, numpy=0.00013986795>]\n",
      "tf.Tensor(0.021825958, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.02167777, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=2.387911e-05>, <tf.Tensor: shape=(), dtype=float32, numpy=0.00011077493>]\n",
      "tf.Tensor(0.021960612, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021990003, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=1.5299147e-05>, <tf.Tensor: shape=(), dtype=float32, numpy=6.892423e-05>]\n",
      "tf.Tensor(0.022044836, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021517629, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=1.1039432e-05>, <tf.Tensor: shape=(), dtype=float32, numpy=5.265764e-05>]\n",
      "tf.Tensor(0.022108532, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021857833, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=8.805112e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=5.1802286e-05>]\n",
      "tf.Tensor(0.02216914, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021843567, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=8.1042135e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=4.5467328e-05>]\n",
      "tf.Tensor(0.02222271, shape=(), dtype=float32)\n",
      "step 10: mean loss = tf.Tensor(0.023967396, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021573503, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=7.0842602e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=3.5503283e-05>]\n",
      "tf.Tensor(0.022265298, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021703996, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=7.5520948e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=2.866567e-05>]\n",
      "tf.Tensor(0.022301516, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021259876, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=6.8976856e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=2.3558527e-05>]\n",
      "tf.Tensor(0.022331972, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021087134, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=5.763024e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=1.7290105e-05>]\n",
      "tf.Tensor(0.022355026, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.022170445, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=8.175771e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=1.48019935e-05>]\n",
      "tf.Tensor(0.022378003, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021568716, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=6.1832056e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=1.2522368e-05>]\n",
      "tf.Tensor(0.022396708, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021366585, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=6.7482083e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=8.760933e-06>]\n",
      "tf.Tensor(0.022412216, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021533985, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=6.1603882e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=6.8118097e-06>]\n",
      "tf.Tensor(0.022425188, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.022179183, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=7.036142e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=6.367499e-06>]\n",
      "tf.Tensor(0.022438591, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021963594, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=5.4626726e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=5.1171846e-06>]\n",
      "tf.Tensor(0.022449171, shape=(), dtype=float32)\n",
      "step 20: mean loss = tf.Tensor(0.022870231, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021978257, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=6.0388506e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=3.842111e-06>]\n",
      "tf.Tensor(0.022459053, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021914406, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=5.2520386e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=3.4079142e-06>]\n",
      "tf.Tensor(0.022467712, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021480842, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=5.3655044e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=2.597142e-06>]\n",
      "tf.Tensor(0.022475675, shape=(), dtype=float32)\n",
      "***********\n",
      "tf.Tensor(0.021934694, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=5.4133125e-06>, <tf.Tensor: shape=(), dtype=float32, numpy=2.4273247e-06>]\n",
      "tf.Tensor(0.022483515, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv2/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-786edd1828ff>\", line 32, in <module>\n",
      "    grads = tape.gradient(loss, vae.trainable_weights)\n",
      "  File \"/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv2/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\", line 1048, in gradient\n",
      "    unconnected_gradients=unconnected_gradients)\n",
      "  File \"/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv2/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\", line 77, in imperative_grad\n",
      "    compat.as_str(unconnected_gradients.value))\n",
      "  File \"/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv2/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\", line 126, in _gradient_function\n",
      "    def _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs,\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv2/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv2/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv2/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv2/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):  \n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    inital_loss = 0\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for batch_arg in range(iterations):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            arg_0 = batch_arg * batch_size\n",
    "            arg_1 = (batch_arg + 1) * batch_size\n",
    "            x_batch, y_batch = x_train[arg_0:arg_1], y_train[arg_0:arg_1]\n",
    "            y_batch = y_batch.astype('float32')\n",
    "            \n",
    "            transformed_image, (z_mean, z_log_var, z_where), clipped_image ,reconstructed_image = vae(x_batch)\n",
    "            \n",
    "#             cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=classification, labels=y_batch)\n",
    "#             logpx_z = -tf.reduce_sum(cross_ent, axis=[1])\n",
    "#             logpz = log_normal_pdf(z_where, 0., 0.)\n",
    "#             logqz_x = log_normal_pdf(z_where, z_mean, z_log_var)\n",
    "#             loss =  -tf.reduce_mean(logpz - logqz_x)\n",
    "            loss = mse_loss_log_fn(x_batch, reconstructed_image)\n",
    "            print('***********')\n",
    "            print(loss)\n",
    "            loss += sum(vae.losses)\n",
    "            \n",
    "            inital_loss += sum(vae.losses)\n",
    "            print(vae.losses)\n",
    "            print(inital_loss)\n",
    "            \n",
    "#             loss = crossentropy_loss_fn(y_batch, classification)\n",
    "#             loss += sum(vae.losses)\n",
    "        \n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "        \n",
    "        loss_metric(loss)\n",
    "\n",
    "        if batch_arg % 10 == 0:\n",
    "          print('step %s: mean loss = %s' % (batch_arg, loss_metric.result()))\n",
    "    print('Epoch loss:{}'.format(inital_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = x_train[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_image, [z_mean, z_log_var, z_where], clipped_image, reconstructed_image = vae(visualize)\n",
    "# transformed_image, [z_mean, z_log_var, z_where], clipped_image, reconstructed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist_sample(transformed_image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist_sample(visualize[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_log_fn(visualize, reconstructed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_log_fn(visualize, reconstructed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist_grid(visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist_grid(transformed_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist_grid(reconstructed_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_latest",
   "language": "python",
   "name": "tf_latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
