{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import pyro\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI, TraceGraph_ELBO\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "import pyro.contrib.examples.multi_mnist as multi_mnist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu, sigmoid, softplus, grid_sample, affine_grid\n",
    "import numpy as np\n",
    "\n",
    "smoke_test = ('CI' in os.environ)\n",
    "assert pyro.__version__.startswith('1.2.1')\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x144 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inpath = './air'\n",
    "X_np, _ = multi_mnist.load(inpath)\n",
    "X_np = X_np.astype(np.float32)\n",
    "X_np /= 255.0\n",
    "mnist = torch.from_numpy(X_np)\n",
    "def show_images(imgs):\n",
    "    figure(figsize=(8, 2))\n",
    "    for i, img in enumerate(imgs):\n",
    "        subplot(1, len(imgs), i + 1)\n",
    "        axis('off')\n",
    "        imshow(img.data.numpy(), cmap='gray')\n",
    "show_images(mnist[9:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network. This takes a latent code, z_what, to pixel intensities.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.l1 = nn.Linear(50, 200)\n",
    "        self.l2 = nn.Linear(200, 400)\n",
    "\n",
    "    def forward(self, z_what):\n",
    "        h = relu(self.l1(z_what))\n",
    "        return sigmoid(self.l2(h))\n",
    "\n",
    "decode = Decoder()\n",
    "\n",
    "z_where_prior_loc = torch.tensor([3., 0., 0.])\n",
    "z_where_prior_scale = torch.tensor([0.1, 1., 1.])\n",
    "z_what_prior_loc = torch.zeros(50)\n",
    "z_what_prior_scale = torch.ones(50)\n",
    "\n",
    "def prior_step_sketch(t):\n",
    "    # Sample object pose. This is a 3-dimensional vector representing x,y position and size.\n",
    "    z_where = pyro.sample('z_where_{}'.format(t),\n",
    "                          dist.Normal(z_where_prior_loc.expand(1, -1),\n",
    "                                      z_where_prior_scale.expand(1, -1))\n",
    "                              .to_event(1))\n",
    "\n",
    "    # Sample object code. This is a 50-dimensional vector.\n",
    "    z_what = pyro.sample('z_what_{}'.format(t),\n",
    "                         dist.Normal(z_what_prior_loc.expand(1, -1),\n",
    "                                     z_what_prior_scale.expand(1, -1))\n",
    "                             .to_event(1))\n",
    "\n",
    "    # Map code to pixel space using the neural network.\n",
    "    y_att = decode(z_what)\n",
    "\n",
    "    # Position/scale object within larger image.\n",
    "    y = object_to_image(z_where, y_att)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_z_where(z_where):\n",
    "    # Takes 3-dimensional vectors, and massages them into 2x3 matrices with elements like so:\n",
    "    # [s,x,y] -> [[s,0,x],\n",
    "    #             [0,s,y]]\n",
    "    n = z_where.size(0)\n",
    "    expansion_indices = torch.LongTensor([1, 0, 2, 0, 1, 3])\n",
    "    out = torch.cat((torch.zeros([1, 1]).expand(n, 1), z_where), 1)\n",
    "    return torch.index_select(out, 1, expansion_indices).view(n, 2, 3)\n",
    "\n",
    "def object_to_image(z_where, obj):\n",
    "    n = obj.size(0)\n",
    "    theta = expand_z_where(z_where)\n",
    "    grid = affine_grid(theta, torch.Size((n, 1, 50, 50)))\n",
    "    out = grid_sample(obj.view(n, 1, 20, 20), grid)\n",
    "    return out.view(n, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_where = pyro.sample('z_where_{}'.format(1),\n",
    "                          dist.Normal(z_where_prior_loc.expand(1, -1),\n",
    "                                      z_where_prior_scale.expand(1, -1))\n",
    "                              .to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0458,  0.1408, -0.5820]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_what = pyro.sample('z_what_{}'.format(1),\n",
    "                         dist.Normal(z_what_prior_loc.expand(1, -1),\n",
    "                                     z_what_prior_scale.expand(1, -1))\n",
    "                             .to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1364,  0.0700,  0.4990,  0.8780,  0.3894,  1.4625,  0.4795, -0.5334,\n",
       "          0.2284,  0.3241, -0.3112, -0.5620, -0.4835, -1.2721, -0.1740,  0.5541,\n",
       "         -1.4928, -1.7644,  0.2942,  0.7973,  1.2642,  0.9355,  0.5455, -1.5374,\n",
       "         -0.5636,  1.6446,  1.4502,  4.1015,  1.1182, -1.5668, -0.6990,  0.5744,\n",
       "          0.6816,  1.5178, -0.0118,  0.9797, -1.0661,  1.7720, -0.2793, -0.2769,\n",
       "          0.7489, -0.6435, -0.9518,  0.2715,  0.6716,  1.8500,  1.1910, -0.5899,\n",
       "          0.9647, -1.5094]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "y_att = decode(z_what)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = y_att.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = expand_z_where(z_where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.0458,  0.0000,  0.1408],\n",
       "         [ 0.0000,  3.0458, -0.5820]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv/lib/python3.7/site-packages/torch/nn/functional.py:2764: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "grid = affine_grid(theta, torch.Size((n, 1, 50, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 50, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv/lib/python3.7/site-packages/torch/nn/functional.py:2705: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "out = grid_sample(y_att.view(n, 1, 20, 20), grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50, 50])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv/lib/python3.7/site-packages/torch/nn/functional.py:2764: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n",
      "/Users/chamathabeysinghe/Projects/monash/test/variational_auto_encoder/venv/lib/python3.7/site-packages/torch/nn/functional.py:2705: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x144 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyro.set_rng_seed(0)\n",
    "samples = [prior_step_sketch(0)[0] for _ in range(5)]\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(0)\n",
    "def geom(num_trials=0):\n",
    "    p = torch.tensor([0.5])\n",
    "    x = pyro.sample('x{}'.format(num_trials), dist.Bernoulli(p))\n",
    "    if x[0] == 1:\n",
    "        return num_trials\n",
    "    else:\n",
    "        return geom(num_trials + 1)\n",
    "\n",
    "# Generate some samples.\n",
    "for _ in range(5):\n",
    "    print('sampled {}'.format(geom()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geom_prior(x, step=0):\n",
    "    p = torch.tensor([0.5])\n",
    "    i = pyro.sample('i{}'.format(step), dist.Bernoulli(p))\n",
    "    if i[0] == 1:\n",
    "        return x\n",
    "    else:\n",
    "        x = x + prior_step_sketch(step)\n",
    "        return geom_prior(x, step + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pyro.set_rng_seed(4)\n",
    "x_empty = torch.zeros(1, 50, 50)\n",
    "samples = [geom_prior(x_empty)[0] for _ in range(5)]\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_step(n, t, prev_x, prev_z_pres):\n",
    "\n",
    "    # Sample variable indicating whether to add this object to the output.\n",
    "\n",
    "    # We multiply the success probability of 0.5 by the value sampled for this\n",
    "    # choice in the previous step. By doing so we add objects to the output until\n",
    "    # the first 0 is sampled, after which we add no further objects.\n",
    "    z_pres = pyro.sample('z_pres_{}'.format(t),\n",
    "                         dist.Bernoulli(0.5 * prev_z_pres)\n",
    "                             .to_event(1))\n",
    "\n",
    "    z_where = pyro.sample('z_where_{}'.format(t),\n",
    "                          dist.Normal(z_where_prior_loc.expand(n, -1),\n",
    "                                      z_where_prior_scale.expand(n, -1))\n",
    "                              .mask(z_pres)\n",
    "                              .to_event(1))\n",
    "\n",
    "    z_what = pyro.sample('z_what_{}'.format(t),\n",
    "                         dist.Normal(z_what_prior_loc.expand(n, -1),\n",
    "                                     z_what_prior_scale.expand(n, -1))\n",
    "                             .mask(z_pres)\n",
    "                             .to_event(1))\n",
    "\n",
    "    y_att = decode(z_what)\n",
    "    y = object_to_image(z_where, y_att)\n",
    "\n",
    "    # Combine the image generated at this step with the image so far.\n",
    "    x = prev_x + y * z_pres.view(-1, 1, 1)\n",
    "\n",
    "    return x, z_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(n):\n",
    "    x = torch.zeros(n, 50, 50)\n",
    "    z_pres = torch.ones(n, 1)\n",
    "    for t in range(3):\n",
    "        x, z_pres = prior_step(n, t, x, z_pres)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(1)\n",
    "show_images(prior(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    # Register network for optimization.\n",
    "    pyro.module(\"decode\", decode)\n",
    "    with pyro.plate('data', data.size(0)) as indices:\n",
    "        batch = data[indices]\n",
    "        x = prior(batch.size(0)).view(-1, 50 * 50)\n",
    "        sd = (0.3 * torch.ones(1)).expand_as(x)\n",
    "        pyro.sample('obs', dist.Normal(x, sd).to_event(1),\n",
    "                    obs=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def guide_step_basic(t, data, prev):\n",
    "\n",
    "    # The RNN takes the images and choices from the previous step as input.\n",
    "    rnn_input = torch.cat((data, prev.z_where, prev.z_what, prev.z_pres), 1)\n",
    "    h, c = rnn(rnn_input, (prev.h, prev.c))\n",
    "\n",
    "    # Compute parameters for all choices made this step, by passing\n",
    "    # the RNN hidden start through another neural network.\n",
    "    z_pres_p, z_where_loc, z_where_scale, z_what_loc, z_what_scale = predict_basic(h)\n",
    "\n",
    "    z_pres = pyro.sample('z_pres_{}'.format(t),\n",
    "                         dist.Bernoulli(z_pres_p * prev.z_pres))\n",
    "\n",
    "    z_where = pyro.sample('z_where_{}'.format(t),\n",
    "                          dist.Normal(z_where_loc, z_where_scale))\n",
    "\n",
    "    z_what = pyro.sample('z_what_{}'.format(t),\n",
    "                         dist.Normal(z_what_loc, z_what_scale))\n",
    "\n",
    "    return # values for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTMCell(2554, 256)\n",
    "\n",
    "# Takes pixel intensities of the attention window to parameters (mean,\n",
    "# standard deviation) of the distribution over the latent code,\n",
    "# z_what.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.l1 = nn.Linear(400, 200)\n",
    "        self.l2 = nn.Linear(200, 100)\n",
    "\n",
    "    def forward(self, data):\n",
    "        h = relu(self.l1(data))\n",
    "        a = self.l2(h)\n",
    "        return a[:, 0:50], softplus(a[:, 50:])\n",
    "\n",
    "encode = Encoder()\n",
    "\n",
    "# Takes the guide RNN hidden state to parameters of\n",
    "# the guide distributions over z_where and z_pres.\n",
    "class Predict(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Predict, self).__init__()\n",
    "        self.l = nn.Linear(256, 7)\n",
    "\n",
    "    def forward(self, h):\n",
    "        a = self.l(h)\n",
    "        z_pres_p = sigmoid(a[:, 0:1]) # Squish to [0,1]\n",
    "        z_where_loc = a[:, 1:4]\n",
    "        z_where_scale = softplus(a[:, 4:]) # Squish to >0\n",
    "        return z_pres_p, z_where_loc, z_where_scale\n",
    "\n",
    "predict = Predict()\n",
    "\n",
    "def guide_step_improved(t, data, prev):\n",
    "\n",
    "    rnn_input = torch.cat((data, prev.z_where, prev.z_what, prev.z_pres), 1)\n",
    "    h, c = rnn(rnn_input, (prev.h, prev.c))\n",
    "    z_pres_p, z_where_loc, z_where_scale = predict(h)\n",
    "\n",
    "    z_pres = pyro.sample('z_pres_{}'.format(t),\n",
    "                         dist.Bernoulli(z_pres_p * prev.z_pres)\n",
    "                             .to_event(1))\n",
    "\n",
    "    z_where = pyro.sample('z_where_{}'.format(t),\n",
    "                          dist.Normal(z_where_loc, z_where_scale)\n",
    "                              .to_event(1))\n",
    "\n",
    "    # New. Crop a small window from the input.\n",
    "    x_att = image_to_object(z_where, data)\n",
    "\n",
    "    # Compute the parameter of the distribution over z_what\n",
    "    # by passing the window through the encoder network.\n",
    "    z_what_loc, z_what_scale = encode(x_att)\n",
    "\n",
    "    z_what = pyro.sample('z_what_{}'.format(t),\n",
    "                         dist.Normal(z_what_loc, z_what_scale)\n",
    "                             .to_event(1))\n",
    "\n",
    "    return # values for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_where_inv(z_where):\n",
    "    # Take a batch of z_where vectors, and compute their \"inverse\".\n",
    "    # That is, for each row compute:\n",
    "    # [s,x,y] -> [1/s,-x/s,-y/s]\n",
    "    # These are the parameters required to perform the inverse of the\n",
    "    # spatial transform performed in the generative model.\n",
    "    n = z_where.size(0)\n",
    "    out = torch.cat((torch.ones([1, 1]).type_as(z_where).expand(n, 1), -z_where[:, 1:]), 1)\n",
    "    out = out / z_where[:, 0:1]\n",
    "    return out\n",
    "\n",
    "def image_to_object(z_where, image):\n",
    "    n = image.size(0)\n",
    "    theta_inv = expand_z_where(z_where_inv(z_where))\n",
    "    grid = affine_grid(theta_inv, torch.Size((n, 1, 20, 20)))\n",
    "    out = grid_sample(image.view(n, 1, 50, 50), grid)\n",
    "    return out.view(n, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_rnn = nn.LSTMCell(2554, 256)\n",
    "bl_predict = nn.Linear(256, 1)\n",
    "\n",
    "# Use an RNN to compute the baseline value. This network takes the\n",
    "# input images and the values samples so far as input.\n",
    "def baseline_step(x, prev):\n",
    "    rnn_input = torch.cat((x,\n",
    "                           prev.z_where.detach(),\n",
    "                           prev.z_what.detach(),\n",
    "                           prev.z_pres.detach()), 1)\n",
    "    bl_h, bl_c = bl_rnn(rnn_input, (prev.bl_h, prev.bl_c))\n",
    "    bl_value = bl_predict(bl_h) * prev.z_pres\n",
    "    return bl_value, bl_h, bl_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GuideState = namedtuple('GuideState', ['h', 'c', 'bl_h', 'bl_c', 'z_pres', 'z_where', 'z_what'])\n",
    "def initial_guide_state(n):\n",
    "    return GuideState(h=torch.zeros(n, 256),\n",
    "                      c=torch.zeros(n, 256),\n",
    "                      bl_h=torch.zeros(n, 256),\n",
    "                      bl_c=torch.zeros(n, 256),\n",
    "                      z_pres=torch.ones(n, 1),\n",
    "                      z_where=torch.zeros(n, 3),\n",
    "                      z_what=torch.zeros(n, 50))\n",
    "\n",
    "def guide_step(t, data, prev):\n",
    "\n",
    "    rnn_input = torch.cat((data, prev.z_where, prev.z_what, prev.z_pres), 1)\n",
    "    h, c = rnn(rnn_input, (prev.h, prev.c))\n",
    "    z_pres_p, z_where_loc, z_where_scale = predict(h)\n",
    "\n",
    "    # Here we compute the baseline value, and pass it to sample.\n",
    "    baseline_value, bl_h, bl_c = baseline_step(data, prev)\n",
    "    z_pres = pyro.sample('z_pres_{}'.format(t),\n",
    "                         dist.Bernoulli(z_pres_p * prev.z_pres)\n",
    "                             .to_event(1),\n",
    "                         infer=dict(baseline=dict(baseline_value=baseline_value.squeeze(-1))))\n",
    "\n",
    "    z_where = pyro.sample('z_where_{}'.format(t),\n",
    "                          dist.Normal(z_where_loc, z_where_scale)\n",
    "                              .mask(z_pres)\n",
    "                              .to_event(1))\n",
    "\n",
    "    x_att = image_to_object(z_where, data)\n",
    "\n",
    "    z_what_loc, z_what_scale = encode(x_att)\n",
    "\n",
    "    z_what = pyro.sample('z_what_{}'.format(t),\n",
    "                         dist.Normal(z_what_loc, z_what_scale)\n",
    "                             .mask(z_pres)\n",
    "                             .to_event(1))\n",
    "\n",
    "    return GuideState(h=h, c=c, bl_h=bl_h, bl_c=bl_c, z_pres=z_pres, z_where=z_where, z_what=z_what)\n",
    "\n",
    "def guide(data):\n",
    "    # Register networks for optimization.\n",
    "    pyro.module('rnn', rnn),\n",
    "    pyro.module('predict', predict),\n",
    "    pyro.module('encode', encode),\n",
    "    pyro.module('bl_rnn', bl_rnn)\n",
    "    pyro.module('bl_predict', bl_predict)\n",
    "\n",
    "    with pyro.plate('data', data.size(0), subsample_size=64) as indices:\n",
    "        batch = data[indices]\n",
    "        state = initial_guide_state(batch.size(0))\n",
    "        steps = []\n",
    "        for t in range(3):\n",
    "            state = guide_step(t, batch, state)\n",
    "            steps.append(state)\n",
    "        return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mnist.view(-1, 50 * 50)\n",
    "\n",
    "svi = SVI(model,\n",
    "          guide,\n",
    "          optim.Adam({'lr': 1e-4}),\n",
    "          loss=TraceGraph_ELBO())\n",
    "\n",
    "for i in range(500):\n",
    "    loss = svi.step(data)\n",
    "    print('i={}, elbo={:.2f}'.format(i, loss / data.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "vae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
